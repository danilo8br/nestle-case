{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "496c87d7-0a33-4b1e-bae7-cb61fda88e53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "<img src=\"/Workspace/Users/danilorock53@gmail.com/nestle.jpg\" alt=\"Texto alternativo\" width=\"1500\">\n",
    "\n",
    "# Projeto Nestle\n",
    "\n",
    "**Descrição:** Este notebook extrai os dados da camada raw e envia para camada bronze em formato `Parquet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f73473a-86b8-4688-b32f-3457c28f61d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Bibliotecas Importadas\n",
    "Importei somente a função col para fazer o processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c829904-dd2e-429a-9700-a78603567676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65685c4e-71f4-4477-82cc-f3a4d5508581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Autenticação no Azure Data Lake Gen2\n",
    "É necessário configurar o acesso ao Azure Data Lake fornecendo a chave da conta de armazenamento:\n",
    "\n",
    "Substitua:\n",
    "- `STORAGE_ACCOUNT` pelo nome da sua conta de armazenamento.\n",
    "\n",
    "- `STORAGE_ACCOUNT_KEY` pela chave de acesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02399dcc-c7d8-4f14-9d01-3eff454a3958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.storagenestlecase.dfs.core.windows.net\", \n",
    "    'chave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecf883bf-86e7-4d39-b086-f2825db160dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[FileInfo(path='abfss://raw@storagenestlecase.dfs.core.windows.net/BaseCEP.csv', name='BaseCEP.csv', size=24695, modificationTime=1737383396000),\n",
       " FileInfo(path='abfss://raw@storagenestlecase.dfs.core.windows.net/BaseCargos.csv', name='BaseCargos.csv', size=1362, modificationTime=1737383396000),\n",
       " FileInfo(path='abfss://raw@storagenestlecase.dfs.core.windows.net/BaseClientes.csv', name='BaseClientes.csv', size=19009, modificationTime=1737383396000),\n",
       " FileInfo(path='abfss://raw@storagenestlecase.dfs.core.windows.net/BaseFuncionarios.csv', name='BaseFuncionarios.csv', size=99764, modificationTime=1737383396000),\n",
       " FileInfo(path='abfss://raw@storagenestlecase.dfs.core.windows.net/BaseNível.csv', name='BaseNível.csv', size=340, modificationTime=1737383396000),\n",
       " FileInfo(path='abfss://raw@storagenestlecase.dfs.core.windows.net/BasePQ.csv', name='BasePQ.csv', size=103983, modificationTime=1737659006000)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listar arquivos do Data Lake\n",
    "dbutils.fs.ls(\"abfss://raw@storagenestlecase.dfs.core.windows.net/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52370640-f756-42d4-bebb-cd5b32da0d93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Leitura e Carga\n",
    "Extração e transformação dos arquivos CSV, com envio para a camada Bronze, onde foram convertidos para o formato Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04926bbe-e30c-4315-a9c7-c196f73da1bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Caminhos dos arquivos no Data Lake Gen 2\n",
    "file_paths = [\n",
    "    \"abfss://camada@storage.dfs.core.windows.net/BaseCargos.csv\",\n",
    "    \"abfss://camada@storage.dfs.core.windows.net/BaseCEP.csv\",\n",
    "    \"abfss://camada@storage.dfs.core.windows.net/BaseClientes.csv\",\n",
    "    \"abfss://camada@storage.dfs.core.windows.net/BaseFuncionarios.csv\",\n",
    "    \"abfss://camada@storage.dfs.core.windows.net/BaseNível.csv\",\n",
    "    \"abfss://camada@storage.dfs.core.windows.net/BasePQ.csv\"\n",
    "]\n",
    "\n",
    "# Leitura dos arquivos CSV\n",
    "base_cargos = spark.read.option(\"delimiter\", \";\").csv(file_paths[0], header=True, inferSchema=True)\n",
    "base_cep = spark.read.option(\"delimiter\", \"|\").csv(file_paths[1], header=True, inferSchema=True)\n",
    "base_clientes = spark.read.option(\"delimiter\", \";\").csv(file_paths[2], header=True, inferSchema=True)\n",
    "base_funcionarios = spark.read.option(\"delimiter\", \"||\").csv(file_paths[3], header=True, inferSchema=True)\n",
    "base_nivel = spark.read.option(\"delimiter\", \"%\").csv(file_paths[4], header=True, inferSchema=True)\n",
    "base_pq = spark.read.option(\"delimiter\", \";\").csv(file_paths[5], header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "# Caminho da camada Bronze no Data Lake Gen2\n",
    "bronze_path = \"abfss://bronze@storagenestlecase.dfs.core.windows.net/bronze/\"\n",
    "\n",
    "# Gravar os DataFrames como Parquet na camada Bronze\n",
    "base_cargos.write.mode(\"overwrite\").parquet(bronze_path + \"BaseCargos.parquet\")\n",
    "base_cep.write.mode(\"overwrite\").parquet(bronze_path + \"BaseCEP.parquet\")\n",
    "base_clientes.write.mode(\"overwrite\").parquet(bronze_path + \"BaseClientes.parquet\")\n",
    "base_funcionarios.write.mode(\"overwrite\").parquet(bronze_path + \"BaseFuncionarios.parquet\")\n",
    "base_nivel.write.mode(\"overwrite\").parquet(bronze_path + \"BaseNivel.parquet\")\n",
    "base_pq.write.mode(\"overwrite\").parquet(bronze_path + \"BasePQ.parquet\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}